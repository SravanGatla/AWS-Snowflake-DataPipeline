{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyME7ySfbJb+QBwB1MbS9HMB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SravanGatla/AWS-Snowflake-DataPipeline/blob/main/processing_and_analytics_practice.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "V7NZ7ML8bs0Q"
      },
      "outputs": [],
      "source": [
        "import boto3\n",
        "import pandas as pd\n",
        "from snowflake.connector import connect, PermissionError\n",
        "from snowflake.connector.pandas_tools import write_pandas\n",
        "\n",
        "\n",
        "class DataExtractor:\n",
        "  def __init__(self, bucket_name, file_key, chunk_size = 10000):\n",
        "    self.bucket_name = bucket_name\n",
        "    self.file_key = file_key # Path of the file/ file which we want to extract.\n",
        "    self.chunk_size = chunk_size\n",
        "\n",
        "  def extract_data(self):\n",
        "    raise NotImplementedError(\"Subclasses must implement extract_data method\")\n",
        "\n",
        "  def create_external_stage(self, conn, stage_name):\n",
        "    cursor = conn.cursor()\n",
        "    create_stage_query = f\"create or replace stage {stage_name}\"\n",
        "    cursor.execute(create_stage_query)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from your_module import DataExtractor\n",
        "\n",
        "class CSVDataExtractor(DataExtractor):\n",
        "  def extract_data(self):\n",
        "    s3 = boto3.client('path of s3')\n",
        "    obj = s3.get_object(Bucket = self.bucket_name, key = self.file_key)\n",
        "    chunks = pd.read_csv(obj['Body'], lines = True, chunksize = self.chunk_size)\n",
        "    data_chunks = []\n",
        "    for i, chunk in enumerate(chunks):\n",
        "      data_chunks.append(chunk)\n",
        "    return data_chunks"
      ],
      "metadata": {
        "id": "kJQbClPZaD_b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from your_module import DataExtractor\n",
        "\n",
        "class JSONDataExtractor(DataExtractor):\n",
        "  def extract_data(self):\n",
        "    s3 = boto3.client('path of s3')\n",
        "    obj = s3.get_object(Bucket = self.bucket_name, file_key = self.file_key)\n",
        "    chunks = pd.read_json(obj['Body'], lines = True, chunks = self.chunk_size)\n",
        "    data_chunks = []\n",
        "    for i, chunk in enumerate(chunks):\n",
        "      data_chunks.append(chunk)\n",
        "    return data_chunks"
      ],
      "metadata": {
        "id": "XYDK1S_2tLJp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from your_module import JSONDataExtractor, CSVDataExtractor\n",
        "\n",
        "class DataProcessor:\n",
        "  def __init__(self, data):\n",
        "    self.data = data\n",
        "\n",
        "  def process_and_analyse_data(self):\n",
        "    self.data['age'] = self.data['age'].astype(int)\n",
        "    self.data['date'] = pd.to_datetime(self.data['date'])\n",
        "    # Map 'gender' values to numeric values\n",
        "    gender_mapping = {\n",
        "            'male': 1,\n",
        "            'female': 0,\n",
        "            'Not Specified': 2\n",
        "        }\n",
        "    self.data['gender'] = self.data['gender'].map(gender_mapping).fillna(self.data['gender'])\n",
        "    return self.data"
      ],
      "metadata": {
        "id": "fxBAzC3kudPI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from your_module import DataProcessor\n",
        "\n",
        "class DataMasker:\n",
        "  def __init__(self, processed_data):\n",
        "    self.processed_data = processed_data\n",
        "\n",
        "  def mask_sensitive_data(self):\n",
        "    def mask_ssn_or_account(x, column_name):\n",
        "      if isinstance(x,str) and column_name in ['SSN', 'account_number']:\n",
        "        return 'X' * len(x)\n",
        "      else:\n",
        "        return x\n",
        "\n",
        "\n",
        "    masked_data = [chunk.applymap(lambda x: mask_ssn_or_account(x, column_name)) for column_name, chunk in self.processed_data.items()]\n",
        "    return masked_data\n"
      ],
      "metadata": {
        "id": "EPYLPp32wuTT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from your_module import DataMasker\n",
        "from snowflake.connector import connect, ProgrammingError\n",
        "from snowflake.connector.pandas_tools import write_pandas\n",
        "\n",
        "class SnowflakeLoader:\n",
        "  def __init__(self, processed_data, snowflake_connection_params, table_name):\n",
        "    self.processed_data = processed_data\n",
        "    self.snowflake_connection_params = snowflake_connection_params\n",
        "    self.table_name = table_name\n",
        "\n",
        "  def load_data_to_snowflake(self):\n",
        "    try:\n",
        "      conn = connect(**self.snowflake_connection_params)\n",
        "      create_stage_query = f\"CREATE OR REPLACE STAGE my_external_stage\"\n",
        "      conn.cursor().execute(create_stage_query)\n",
        "\n",
        "      for i, chunk in enumerate(self.processed_data):\n",
        "        stage_file_path = f\"/data_chunk_{i}.csv\"\n",
        "        write_pandas(conn, chunk, stage_name = 'my_external_stage', table_name = stage_file_path)\n",
        "\n",
        "      copy_into_table_query = f\"COPY INTO {self.table_name} FROM '@my_external_stage' FILE_FORMAT = (TYPE = CSV)\"\n",
        "      conn.cursor().execute(copy_into_table_query)\n",
        "      conn.commit()\n",
        "      print(\"Data Loaded into Snowflake successfully.\")\n",
        "    except ProgrammingError as e:\n",
        "     print(\"Error:\", e)\n",
        "\n"
      ],
      "metadata": {
        "id": "Wzad0A7PDjM7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from your_module import process_and_analyse_data, mask_sensitive_data, load_data_to_snowflake\n",
        "\n",
        "def process_data_in_parallel(process, data):\n",
        "  return process.process_and_analyse_data(data)\n",
        "\n",
        "def mask_data_in_parallel(process, data):\n",
        "  return process.mask_sensitive_data(data)\n",
        "\n",
        "def load_data_in_parallel(process, data):\n",
        "  return process.load_data_in_parallel(data)\n"
      ],
      "metadata": {
        "id": "fXqI7G11e6nv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import boto3\n",
        "import pandas as pd\n",
        "import multiprocessing\n",
        "from snowflake.connector import connect,ProgrammingError\n",
        "from your_module import CSVDataExtractor, JSONDataExtractor\n",
        "from your_module import process_and_analyse_data, mask_sensitive_data, load_data_to_snowflake\n",
        "from your_module import process_data_in_parallel, mask_data_in_parallel, load_data_in_parallel\n",
        "\n",
        "def main():\n",
        "  s3_bucket_name = 'name of the S3 bucket'\n",
        "  S3_file_key = 'path/your_file_key.csv'\n",
        "  snowflake_connection_params = {\n",
        "      'user': 'your-username',\n",
        "      'password': 'your-password',\n",
        "      'account': 'your-account',\n",
        "      'warehouse': 'your-warehouse',\n",
        "      'database': 'your-database',\n",
        "      'schema': 'your-schema'\n",
        "  }\n",
        "  table_name = 'your-table-name'\n",
        "\n",
        "  extractor = CSVDataExtractor(s3_bucket_name, S3_file_key)\n",
        "  data_chunks = extractor.extract_data()\n",
        "\n",
        "  processors = [DataProcessor(chunk) for chunk in data_chunks]\n",
        "  with multiprocessing.Pool() as pool:\n",
        "    processed_chunks = pool.starmap(process_data_in_parallel, zip(processors, data_chunks))\n",
        "\n",
        "  maskers = [DataMasker(chunk) for chunk in processed_chunks]\n",
        "  with multiprocessing.Pool() as pool:\n",
        "    masked_chunks = pool.starmap(mask_data_in_parallel, zip(maskers, processed_chunks))\n",
        "\n",
        "  snowflake_loaders = [SnowflakeLoader(chunk, snowflake_connection_params, table_name) for chunk in masked_chunks]\n",
        "  with multiprocessing.Pool() as pool:\n",
        "    pool.map(load_data_in_parallel, snowflake_loaders)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  main()"
      ],
      "metadata": {
        "id": "7bUncTYHhSuA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}